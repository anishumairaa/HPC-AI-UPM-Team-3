Wed Nov  6 20:26:43 +08 2024
CONDA_SHLVL=0
PBS_ENVIRONMENT=PBS_BATCH
LD_LIBRARY_PATH=/opt/cray/libfabric/1.11.0.4.125/lib64:/app/apps/openmpi/4.1.2-hpe/lib:/app/apps/local/lib64:/app/apps/local/lib
LS_COLORS=rs=0:di=38;5;33:ln=38;5;51:mh=00:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=01;05;37;41:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;40:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.zst=38;5;9:*.tzst=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.wim=38;5;9:*.swm=38;5;9:*.dwm=38;5;9:*.esd=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.mjpg=38;5;13:*.mjpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.m4a=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.oga=38;5;45:*.opus=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:
CONDA_EXE=/home/users/industry/ai-hpc/apacsc32/miniconda/bin/conda
nvidia_group_id=227
PBS_O_LANG=en_US.UTF-8
PRGENVMODULES=PrgEnv-aocc:PrgEnv-cray:PrgEnv-gnu:PrgEnv-intel:PrgEnv-nvidia:PrgEnv-pgi:PrrgEnv-amd
max_steps=20
SSH_CONNECTION=10.103.62.1 42042 10.103.153.100 22
LANG=en_US.UTF-8
TZ=Asia/Singapore
HISTCONTROL=ignoredups
HOSTNAME=x1000c3s3b0n0
OLDPWD=/home/users/industry/ai-hpc/apacsc32
PBS_O_HOME=/home/users/industry/ai-hpc/apacsc32
PBS_JOBID=8613326.pbs101
ENVIRONMENT=BATCH
global_batch_size=128
PBS_JOBNAME=llama.nodes2.GBS128.MBS32
FPATH=:/opt/cray/pe/modules/3.2.11.6/init/sh_funcs/no_redirect:/opt/cray/pe/modules/3.2.11.6/init/sh_funcs/no_redirect
NCPUS=64
PBS_O_PATH=/home/users/industry/ai-hpc/apacsc32/.local/bin:/home/users/industry/ai-hpc/apacsc32/miniconda/condabin:/home/users/industry/ai-hpc/apacsc32/.local/bin:/home/users/industry/ai-hpc/apacsc32/bin:/opt/cray/pe/pals/1.1.6/bin:/opt/cray/pe/craype/2.7.15/bin:/opt/cray/pe/cce/13.0.2/binutils/x86_64/x86_64-pc-linux-gnu/bin:/opt/cray/pe/cce/13.0.2/binutils/cross/x86_64-aarch64/aarch64-linux-gnu/../bin:/opt/cray/pe/cce/13.0.2/utils/x86_64/bin:/opt/cray/pe/cce/13.0.2/bin:/opt/cray/pe/perftools/22.04.0/bin:/opt/cray/pe/papi/6.0.0.14/bin:/opt/cray/libfabric/1.11.0.4.125/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/app/apps/local/bin:/opt/c3/bin:/usr/lpp/mmfs/bin:/opt/pbs/bin:/sbin:/bin:/opt/cray/pe/bin
S_COLORS=auto
C3_RSH=ssh -oConnectTimeout=10 -oForwardX11=no
_CE_M=
XDG_SESSION_ID=103139
PE_OPENMPI_BASEDIR=/app/apps/openmpi/4.1.2-hpe
PBS_O_WORKDIR=/home/users/industry/ai-hpc/apacsc32/run
USER=apacsc32
MODULE_VERSION=3.2.11.6
TOOLMODULES=apprentice:apprentice2:atp:ccdb:cdt:chapel:cray-cti:cray-lgdb:craypat:craypkg-gen:cray-R:cray-snplauncher:ddt:gdb:gdb4hpc:iobuf:papi:perftools:perftools-lite:python:stat:totalview:xt-craypat:xt-lgdb:xt-papi:xt-totalview
micro_batch_size=32
PBS_NODEFILE=/var/spool/pbs/aux/8613326.pbs101
KSH_AUTOLOAD=1
PE_FORTRAN_PKGCONFIG_LIBS=ompi-fort
PBS_TASKNUM=1
PWD=/home/users/industry/ai-hpc/apacsc32
TARGETMODULES=craype-accel-amd-gfx908:craype-accel-amd-gfx90a:craype-accel-host:craype-accel-nvidia70:craype-accel-nvidia80:craype-network-none:craype-network-ofi:craype-network-ucx:craype-x86-milan:craype-x86-milan-x:craype-x86-rome:craype-x86-trento
HOME=/home/users/industry/ai-hpc/apacsc32
CONDA_PYTHON_EXE=/home/users/industry/ai-hpc/apacsc32/miniconda/bin/python
PELOCAL_PRGENV=true
SSH_CLIENT=10.103.62.1 42042 22
CPATH=/app/apps/openmpi/4.1.2-hpe/include
PBS_MOMPORT=15003
OPENMPI_VERSION=4.1.2-hpe
XDG_DATA_DIRS=/home/users/industry/ai-hpc/apacsc32/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share
PE_CXX_PKGCONFIG_LIBS=ompi-cxx
_CE_CONDA=
PBS_JOBCOOKIE=1CF357863E3929FE5F7BEC725F0EAF99
PBS_O_SHELL=/bin/bash
TMPDIR=/var/tmp/pbs.8613326.pbs101
LIBRARY_PATH=/app/apps/openmpi/4.1.2-hpe/lib
PE_PKGCONFIG_LIBS=ompi
USERMODULES=acml:alps:apprentice:apprentice2:atp:blcr:cce:chapel:cray-ccdb:cray-fftw:cray-ga:cray-hdf5:cray-hdf5-parallel:cray-lgdb:cray-libsci:cray-libsci_acc:cray-mpich:cray-mpich2:cray-mpich-compat:cray-netcdf:cray-netcdf-hdf5parallel:cray-openshmemx:cray-parallel-netcdf:craypat:craype:cray-petsc:cray-petsc-complex:craypkg-gen:cray-R:cray-shmem:cray-snplauncher:cray-tpsl:cray-trilinos:cudatoolkit:ddt:fftw:ga:gcc:hdf5:hdf5-parallel:intel:iobuf:java:lgdb:libfast:libsci_acc:mpich1:netcdf:netcdf-hdf5parallel:netcdf-nofsync:netcdf-nofsync-hdf5parallel:ntk:nvidia:onesided:papi:parallel-netcdf:pathscale:perftools:perftools-lite:petsc:petsc-complex:pgi:pmi:PrgEnv-amd:PrgEnv-aocc:PrgEnv-cray:PrgEnv-gnu:PrgEnv-intel:PrgEnv-nvidia:PrgEnv-pathscale:PrgEnv-pgi:python:rocm-compiler:stat:totalview:tpsl:trilinos:xt-asyncpe:xt-craypat:xt-lgdb:xt-libsci:xt-mpich2:xt-mpt:xt-papi:xt-shmem:xt-totalview
LOADEDMODULES=openmpi/4.1.2-hpe:libfabric/1.11.0.4.125
LIBRARYMODULES=acml:alps:cray-dwarf:cray-fftw:cray-ga:cray-hdf5:cray-hdf5-parallel:cray-libsci:cray-libsci_acc:cray-mpich:cray-mpich2:cray-mpich-abi:cray-netcdf:cray-netcdf-hdf5parallel:cray-parallel-netcdf:cray-petsc:cray-petsc-complex:cray-shmem:cray-tpsl:cray-trilinos:cudatoolkit:fftw:ga:hdf5:hdf5-parallel:iobuf:libfast:netcdf:netcdf-hdf5parallel:ntk:onesided:papi:petsc:petsc-complex:pmi:tpsl:trilinos:xt-libsci:xt-mpich2:xt-mpt:xt-papi
SSH_TTY=/dev/pts/31
PBS_O_QUEUE=normal
MAIL=/var/spool/mail/apacsc32
MOTDLOCK=1
CRAY_SITE_LIST_DIR=/etc/cray-pe.d/cray-modules
SHELL=/bin/bash
TERM=xterm-256color
walltime=7201
USE_PCM_DB=2
CUDA_DEVICE_ORDER=PCI_BUS_ID
CUDA_VISIBLE_DEVICES=GPU-561c6b71-7343-fa1e-c211-2b1b4dd801ae,GPU-0b3f2349-175d-c634-9e22-a5be67bade65,GPU-f139556b-2534-026e-1e05-489aaccc5ced,GPU-8c3e3518-6eee-d5f5-535a-c13f46263590
SHLVL=4
LANGUAGE=en_US.UTF-8
PBS_O_HOST=asp2a-login-nscc01.head.cm.asp2a.nscc.sg
PBS_O_SYSTEM=Linux
MANPATH=/opt/cray/libfabric/1.11.0.4.125/share/man:/app/apps/openmpi/4.1.2-hpe/share/man:/opt/cray/pe/modules/3.2.11.6/share/man:/opt/cray/pe/modules/3.2.11.6/share/man::/opt/c3/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man:/opt/pbs/share/man:/opt/clmgr/man:/opt/sgi/share/man:/opt/clmgr/share/man:/opt/clmgr/lib/cm-cli/man
OSCAR_HOME=/opt/oscar
PBS_O_LOGNAME=apacsc32
PBS_NODENUM=0
MODULEPATH=/opt/cray/pe/modulefiles:/opt/cray/modulefiles:/opt/modulefiles:/opt/cray/pe/craype-targets/default/modulefiles:/app/apps/modulefiles:/app/libs/modulefiles
PBS_JOBDIR=/home/users/industry/ai-hpc/apacsc32
LOGNAME=apacsc32
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/11530/bus
XDG_RUNTIME_DIR=/run/user/11530
nodes=2
MODULE_VERSION_STACK=3.2.11.6
PE_C_PKGCONFIG_LIBS=ompi-c
OPENMPI_DIR=/app/apps/openmpi/4.1.2-hpe
PATH=/opt/cray/libfabric/1.11.0.4.125/bin:/app/apps/openmpi/4.1.2-hpe/bin:/app/apps/openmpi/4.1.2-hpe/include:/home/users/industry/ai-hpc/apacsc32/.local/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/home/users/industry/ai-hpc/apacsc32/.local/bin:/home/users/industry/ai-hpc/apacsc32/miniconda/condabin:/home/users/industry/ai-hpc/apacsc32/.local/bin:/home/users/industry/ai-hpc/apacsc32/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/app/apps/local/bin:/opt/c3/bin:/usr/lpp/mmfs/bin:/opt/pbs/bin:/sbin:/bin:/opt/pbs/bin:/sbin:/bin
_LMFILES_=/app/apps/modulefiles/openmpi/4.1.2-hpe:/opt/cray/modulefiles/libfabric/1.11.0.4.125
PBS_QUEUE=g4
MODULESHOME=/opt/cray/pe/modules/3.2.11.6
PKG_CONFIG_PATH=/opt/cray/libfabric/1.11.0.4.125/lib64/pkgconfig:/app/apps/openmpi/4.1.2-hpe/lib/pkgconfig
HISTSIZE=1000
OPENMPI_POST_LINK_OPTS=-L/app/apps/openmpi/4.1.2-hpe/lib -Wl,-rpath=/app/apps/openmpi/4.1.2-hpe/lib
PBS_O_MAIL=/var/spool/mail/apacsc32
LESSOPEN=||/usr/bin/lesspipe.sh %s
OMP_NUM_THREADS=64
LC_TIME=en_US.UTF-8
BASH_FUNC_module%%=() {  eval `/opt/cray/pe/modules/3.2.11.6/bin/modulecmd bash $*`
}
_=/usr/bin/env
x1000c3s3b0n0.hostmgmt2000.cm.asp2a.nscc.sg
x1000c3s4b0n0.hostmgmt2000.cm.asp2a.nscc.sg
Wed Nov  6 20:26:43 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:03:00.0 Off |                    0 |
| N/A   43C    P0              52W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:41:00.0 Off |                    0 |
| N/A   42C    P0              51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:81:00.0 Off |                    0 |
| N/A   43C    P0              51W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:C1:00.0 Off |                    0 |
| N/A   43C    P0              54W / 400W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
mpirun -wdir /home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama -output-filename /home/users/industry/ai-hpc/apacsc32/run/output/llama.nodes2.GBS128.MBS32.8613326.pbs101 -map-by ppr:4:node -oversubscribe -report-bindings -mca coll_hcoll_enable 1 -mca coll_basic_priority 10 -x mpirun -mca pml ^ucx /home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/bin/litgpt finetune_full /home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf --data JSON --data.json_path /home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024 --out_dir /home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full --config /home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/full.yaml --eval.final_validation=false --train.epochs=1 --devices=4 --num_nodes=2 --train.max_steps=20 --train.global_batch_size=128 --train.micro_batch_size=32
[x1000c3s3b0n0:828713] Warning: could not find environment variable "mpirun"
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1541204e9cd0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14d6e16d5cd0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1456593497c0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x14676aba0d10>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
[x1000c3s3b0n0:828731] MCW rank 3 is not bound (or bound to all available processors)
[x1000c3s3b0n0:828728] MCW rank 0 is not bound (or bound to all available processors)
[x1000c3s3b0n0:828729] MCW rank 1 is not bound (or bound to all available processors)
[x1000c3s3b0n0:828730] MCW rank 2 is not bound (or bound to all available processors)
--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           x1000c3s3b0n0
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x154f621d3830>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
{'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1475b7c29fd0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
{'access_token': None,
 /home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 20 max_steps.
  warnings.warn(
'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024'),
              {'access_token': None,
 'checkpoint_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/model/litgpt/meta-llama/Llama-2-7b-hf'),
 'data': JSON(json_path=PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/dataset/alpaca1024'),
              mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1532b731eed0>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
mask_prompt=False,
              val_split_fraction=None,
              prompt_style=<litgpt.prompts.Alpaca object at 0x1506b6624950>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 4,
 'eval': EvalArgs(interval=25000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=False),
 'logger_name': 'csv',
 'num_nodes': 2,
 'optimizer': 'AdamW',
 'out_dir': PosixPath('/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/out/finetune/full'),
 'precision': 'bf16-true',
 'resume': False,
 'seed': 1337,
 'train': TrainArgs(save_interval=20000,
                    log_interval=1,
                    global_batch_size=128,
                    micro_batch_size=32,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=1,
                    max_tokens=None,
                    max_steps=20,
                    max_seq_length=512,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-05)}
[x1000c3s4b0n0:2898257] MCW rank 6 is not bound (or bound to all available processors)
[x1000c3s4b0n0:2898258] MCW rank 7 is not bound (or bound to all available processors)
[x1000c3s4b0n0:2898256] MCW rank 5 is not bound (or bound to all available processors)
[x1000c3s4b0n0:2898255] MCW rank 4 is not bound (or bound to all available processors)
All GPUs are fully connected via NVLink.
All GPUs are fully connected via NVLink.
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

Seed set to 1337
Seed set to 1337
Seed set to 1337
Number of trainable parameters: 6,738,415,616
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Seed set to 1337
Number of trainable parameters: 6,738,415,616
x1000c3s3b0n0:828728:828728 [0] NCCL INFO Bootstrap : Using hsn0:10.150.1.226<0>
x1000c3s3b0n0:828728:828728 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
x1000c3s3b0n0:828728:828728 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
x1000c3s3b0n0:828728:828728 [0] NCCL INFO NET/Plugin: Using internal network plugin.
x1000c3s3b0n0:828728:828728 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.21.5+cuda12.4
x1000c3s3b0n0:828731:828731 [3] NCCL INFO cudaDriverVersion 12020
x1000c3s3b0n0:828730:828730 [2] NCCL INFO cudaDriverVersion 12020
x1000c3s3b0n0:828729:828729 [1] NCCL INFO cudaDriverVersion 12020
x1000c3s4b0n0:2898257:2898257 [2] NCCL INFO cudaDriverVersion 12020
x1000c3s3b0n0:828730:828730 [2] NCCL INFO Bootstrap : Using hsn0:10.150.1.226<0>
x1000c3s3b0n0:828731:828731 [3] NCCL INFO Bootstrap : Using hsn0:10.150.1.226<0>
x1000c3s3b0n0:828729:828729 [1] NCCL INFO Bootstrap : Using hsn0:10.150.1.226<0>
x1000c3s4b0n0:2898257:2898257 [2] NCCL INFO Bootstrap : Using hsn0:10.150.1.234<0>
x1000c3s3b0n0:828731:828731 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
x1000c3s3b0n0:828731:828731 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
x1000c3s3b0n0:828731:828731 [3] NCCL INFO NET/Plugin: Using internal network plugin.
x1000c3s3b0n0:828730:828730 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
x1000c3s3b0n0:828730:828730 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
x1000c3s3b0n0:828730:828730 [2] NCCL INFO NET/Plugin: Using internal network plugin.
x1000c3s3b0n0:828729:828729 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
x1000c3s3b0n0:828729:828729 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
x1000c3s3b0n0:828729:828729 [1] NCCL INFO NET/Plugin: Using internal network plugin.
x1000c3s4b0n0:2898256:2898256 [1] NCCL INFO cudaDriverVersion 12020
x1000c3s4b0n0:2898257:2898257 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
x1000c3s4b0n0:2898257:2898257 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
x1000c3s4b0n0:2898257:2898257 [2] NCCL INFO NET/Plugin: Using internal network plugin.
x1000c3s4b0n0:2898256:2898256 [1] NCCL INFO Bootstrap : Using hsn0:10.150.1.234<0>
x1000c3s4b0n0:2898256:2898256 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
x1000c3s4b0n0:2898256:2898256 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
x1000c3s4b0n0:2898256:2898256 [1] NCCL INFO NET/Plugin: Using internal network plugin.
x1000c3s4b0n0:2898258:2898258 [3] NCCL INFO cudaDriverVersion 12020
x1000c3s4b0n0:2898258:2898258 [3] NCCL INFO Bootstrap : Using hsn0:10.150.1.234<0>
x1000c3s4b0n0:2898258:2898258 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
x1000c3s4b0n0:2898258:2898258 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
x1000c3s4b0n0:2898258:2898258 [3] NCCL INFO NET/Plugin: Using internal network plugin.
x1000c3s4b0n0:2898255:2898255 [0] NCCL INFO cudaDriverVersion 12020
x1000c3s4b0n0:2898255:2898255 [0] NCCL INFO Bootstrap : Using hsn0:10.150.1.234<0>
x1000c3s4b0n0:2898255:2898255 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
x1000c3s4b0n0:2898255:2898255 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
x1000c3s4b0n0:2898255:2898255 [0] NCCL INFO NET/Plugin: Using internal network plugin.
x1000c3s3b0n0:828728:829302 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c3s3b0n0:828728:829302 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.226<0> [1]hsn1:10.150.1.227<0> [2]bond0:10.168.0.54<0>
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Using network Socket
x1000c3s3b0n0:828729:829306 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c3s3b0n0:828731:829305 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c3s3b0n0:828729:829306 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.226<0> [1]hsn1:10.150.1.227<0> [2]bond0:10.168.0.54<0>
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Using network Socket
x1000c3s3b0n0:828730:829304 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c3s3b0n0:828731:829305 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.226<0> [1]hsn1:10.150.1.227<0> [2]bond0:10.168.0.54<0>
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Using network Socket
x1000c3s3b0n0:828730:829304 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.226<0> [1]hsn1:10.150.1.227<0> [2]bond0:10.168.0.54<0>
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Using non-device net plugin version 0
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Using network Socket
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.234<0> [1]hsn1:10.150.1.235<0> [2]bond0:10.168.0.56<0>
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.234<0> [1]hsn1:10.150.1.235<0> [2]bond0:10.168.0.56<0>
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.234<0> [1]hsn1:10.150.1.235<0> [2]bond0:10.168.0.56<0>
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Using non-device net plugin version 0
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Using network Socket
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Using non-device net plugin version 0
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Using network Socket
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Using non-device net plugin version 0
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Using network Socket
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 1.
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO NET/Socket : Using [0]hsn0:10.150.1.234<0> [1]hsn1:10.150.1.235<0> [2]bond0:10.168.0.56<0>
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Using non-device net plugin version 0
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Using network Socket
x1000c3s3b0n0:828728:829302 [0] NCCL INFO ncclCommInitRank comm 0x161490e0 rank 0 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x17a6476d46efe9d9 - Init START
x1000c3s3b0n0:828731:829305 [3] NCCL INFO ncclCommInitRank comm 0x14117290 rank 3 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x17a6476d46efe9d9 - Init START
x1000c3s3b0n0:828730:829304 [2] NCCL INFO ncclCommInitRank comm 0x1558c8c0 rank 2 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x17a6476d46efe9d9 - Init START
x1000c3s3b0n0:828729:829306 [1] NCCL INFO ncclCommInitRank comm 0x1413d660 rank 1 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x17a6476d46efe9d9 - Init START
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO ncclCommInitRank comm 0x143701b0 rank 6 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x17a6476d46efe9d9 - Init START
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO ncclCommInitRank comm 0x14a1e7b0 rank 7 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x17a6476d46efe9d9 - Init START
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO ncclCommInitRank comm 0x152e6ff0 rank 5 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x17a6476d46efe9d9 - Init START
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO ncclCommInitRank comm 0x14f65fa0 rank 4 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x17a6476d46efe9d9 - Init START
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Setting affinity for GPU 3 to ffff,00000000,0000ffff
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO NVLS multicast support is not available on dev 1
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000
x1000c3s3b0n0:828730:829304 [2] NCCL INFO NVLS multicast support is not available on dev 2
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
x1000c3s3b0n0:828728:829302 [0] NCCL INFO NVLS multicast support is not available on dev 0
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Setting affinity for GPU 1 to ffff,00000000,0000ffff,00000000
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO NVLS multicast support is not available on dev 0
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
x1000c3s3b0n0:828731:829305 [3] NCCL INFO NVLS multicast support is not available on dev 3
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,ffff0000,00000000
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO NVLS multicast support is not available on dev 2
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Setting affinity for GPU 3 to ffff,00000000,0000ffff
x1000c3s3b0n0:828729:829306 [1] NCCL INFO NVLS multicast support is not available on dev 1
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,ffff0000
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO NVLS multicast support is not available on dev 3
x1000c3s3b0n0:828731:829305 [3] NCCL INFO comm 0x14117290 rank 3 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
x1000c3s3b0n0:828731:829305 [3] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:828728:829302 [0] NCCL INFO comm 0x161490e0 rank 0 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 00/04 :    0   3   6   5   4   7   2   1
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 01/04 :    0   5   7   6   4   1   3   2
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 02/04 :    0   3   6   5   4   7   2   1
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 03/04 :    0   5   7   6   4   1   3   2
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] 1/4/-1->0->-1 [2] 1/-1/-1->0->4 [3] 1/-1/-1->0->4
x1000c3s3b0n0:828728:829302 [0] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:828730:829304 [2] NCCL INFO comm 0x1558c8c0 rank 2 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
x1000c3s3b0n0:828730:829304 [2] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:828729:829306 [1] NCCL INFO comm 0x1413d660 rank 1 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
x1000c3s3b0n0:828729:829306 [1] NCCL INFO P2P Chunksize set to 131072
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO comm 0x152e6ff0 rank 5 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO P2P Chunksize set to 131072
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO comm 0x14f65fa0 rank 4 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] 5/-1/-1->4->0 [2] 5/0/-1->4->-1 [3] 5/0/-1->4->-1
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO P2P Chunksize set to 131072
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO comm 0x143701b0 rank 6 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO P2P Chunksize set to 131072
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO comm 0x14a1e7b0 rank 7 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO P2P Chunksize set to 131072
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 00/0 : 0[1] -> 3[2] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 01/0 : 1[3] -> 3[2] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 01/0 : 5[3] -> 7[2] via P2P/CUMEM/read
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 00/0 : 4[1] -> 7[2] via P2P/CUMEM/read
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 02/0 : 0[1] -> 3[2] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 03/0 : 1[3] -> 3[2] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 03/0 : 5[3] -> 7[2] via P2P/CUMEM/read
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 02/0 : 4[1] -> 7[2] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 00/0 : 3[2] -> 6[0] [receive] via NET/Socket/1
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 00/0 : 7[2] -> 2[0] [receive] via NET/Socket/1
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 02/0 : 3[2] -> 6[0] [receive] via NET/Socket/1
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 01/0 : 4[1] -> 1[3] [receive] via NET/Socket/0
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 01/0 : 0[1] -> 5[3] [receive] via NET/Socket/0
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 02/0 : 7[2] -> 2[0] [receive] via NET/Socket/1
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Channel 00/0 : 7[2] -> 2[0] [send] via NET/Socket/1
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 03/0 : 4[1] -> 1[3] [receive] via NET/Socket/0
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 03/0 : 0[1] -> 5[3] [receive] via NET/Socket/0
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Channel 00/0 : 3[2] -> 6[0] [send] via NET/Socket/1
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Channel 02/0 : 7[2] -> 2[0] [send] via NET/Socket/1
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 01/0 : 2[0] -> 0[1] via P2P/CUMEM/read
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Channel 01/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Channel 02/0 : 3[2] -> 6[0] [send] via NET/Socket/1
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 03/0 : 2[0] -> 0[1] via P2P/CUMEM/read
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Channel 03/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Channel 01/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 01/0 : 6[0] -> 4[1] via P2P/CUMEM/read
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Channel 03/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 03/0 : 6[0] -> 4[1] via P2P/CUMEM/read
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 01/0 : 4[1] -> 1[3] [send] via NET/Socket/0
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 03/0 : 4[1] -> 1[3] [send] via NET/Socket/0
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 01/0 : 0[1] -> 5[3] [send] via NET/Socket/0
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 00/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 02/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 03/0 : 0[1] -> 5[3] [send] via NET/Socket/0
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 00/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 02/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 00/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 02/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 00/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 02/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Connected all rings
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 00/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Connected all rings
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Connected all rings
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 00/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Connected all rings
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Connected all rings
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Connected all rings
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Connected all rings
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Connected all rings
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 01/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 01/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 02/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 03/0 : 0[1] -> 1[3] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 00/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 00/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 00/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 02/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 01/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 01/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 01/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 03/0 : 4[1] -> 5[3] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 02/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 02/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 02/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 03/0 : 1[3] -> 2[0] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 03/0 : 6[0] -> 7[2] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 03/0 : 2[0] -> 3[2] via P2P/CUMEM/read
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Channel 00/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Channel 02/0 : 3[2] -> 2[0] via P2P/CUMEM/read
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Channel 00/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Channel 02/0 : 7[2] -> 6[0] via P2P/CUMEM/read
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 01/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 01/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 00/0 : 4[1] -> 0[1] [receive] via NET/Socket/1
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Channel 03/0 : 1[3] -> 0[1] via P2P/CUMEM/read
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Channel 03/0 : 2[0] -> 1[3] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 00/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 01/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 02/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 03/0 : 5[3] -> 6[0] via P2P/CUMEM/read
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 01/0 : 4[1] -> 0[1] [receive] via NET/Socket/0
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 01/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 02/0 : 4[1] -> 0[1] [receive] via NET/Socket/1
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 01/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Channel 03/0 : 6[0] -> 5[3] via P2P/CUMEM/read
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Channel 03/0 : 5[3] -> 4[1] via P2P/CUMEM/read
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 00/0 : 0[1] -> 4[1] [receive] via NET/Socket/1
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 03/0 : 4[1] -> 0[1] [receive] via NET/Socket/0
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 01/0 : 0[1] -> 4[1] [receive] via NET/Socket/0
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 00/0 : 0[1] -> 4[1] [send] via NET/Socket/1
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 02/0 : 0[1] -> 4[1] [receive] via NET/Socket/1
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 01/0 : 0[1] -> 4[1] [send] via NET/Socket/0
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 03/0 : 0[1] -> 4[1] [receive] via NET/Socket/0
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 02/0 : 0[1] -> 4[1] [send] via NET/Socket/1
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 00/0 : 4[1] -> 0[1] [send] via NET/Socket/1
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Channel 03/0 : 0[1] -> 4[1] [send] via NET/Socket/0
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 01/0 : 4[1] -> 0[1] [send] via NET/Socket/0
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 02/0 : 4[1] -> 0[1] [send] via NET/Socket/1
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Channel 03/0 : 4[1] -> 0[1] [send] via NET/Socket/0
x1000c3s3b0n0:828731:829305 [3] NCCL INFO Connected all trees
x1000c3s3b0n0:828731:829305 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c3s3b0n0:828731:829305 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO Connected all trees
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO Connected all trees
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c3s3b0n0:828730:829304 [2] NCCL INFO Connected all trees
x1000c3s3b0n0:828730:829304 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c3s3b0n0:828730:829304 [2] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c3s3b0n0:828729:829306 [1] NCCL INFO Connected all trees
x1000c3s3b0n0:828729:829306 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c3s3b0n0:828729:829306 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c3s3b0n0:828728:829302 [0] NCCL INFO Connected all trees
x1000c3s3b0n0:828728:829302 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c3s3b0n0:828728:829302 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO Connected all trees
x1000c3s3b0n0:828730:829304 [2] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
x1000c3s3b0n0:828730:829304 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
x1000c3s3b0n0:828730:829304 [2] NCCL INFO ncclCommInitRank comm 0x1558c8c0 rank 2 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x17a6476d46efe9d9 - Init COMPLETE
x1000c3s3b0n0:828731:829305 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
x1000c3s3b0n0:828731:829305 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
x1000c3s3b0n0:828731:829305 [3] NCCL INFO ncclCommInitRank comm 0x14117290 rank 3 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x17a6476d46efe9d9 - Init COMPLETE
x1000c3s3b0n0:828729:829306 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
x1000c3s3b0n0:828729:829306 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
x1000c3s3b0n0:828729:829306 [1] NCCL INFO ncclCommInitRank comm 0x1413d660 rank 1 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x17a6476d46efe9d9 - Init COMPLETE
x1000c3s3b0n0:828728:829302 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
x1000c3s3b0n0:828728:829302 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
x1000c3s3b0n0:828728:829302 [0] NCCL INFO ncclCommInitRank comm 0x161490e0 rank 0 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x17a6476d46efe9d9 - Init COMPLETE
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO Connected all trees
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
x1000c3s4b0n0:2898257:2898871 [2] NCCL INFO ncclCommInitRank comm 0x143701b0 rank 6 nranks 8 cudaDev 2 nvmlDev 0 busId 3000 commId 0x17a6476d46efe9d9 - Init COMPLETE
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
x1000c3s4b0n0:2898258:2898873 [3] NCCL INFO ncclCommInitRank comm 0x14a1e7b0 rank 7 nranks 8 cudaDev 3 nvmlDev 2 busId 81000 commId 0x17a6476d46efe9d9 - Init COMPLETE
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
x1000c3s4b0n0:2898256:2898872 [1] NCCL INFO ncclCommInitRank comm 0x152e6ff0 rank 5 nranks 8 cudaDev 1 nvmlDev 3 busId c1000 commId 0x17a6476d46efe9d9 - Init COMPLETE
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
x1000c3s4b0n0:2898255:2898874 [0] NCCL INFO ncclCommInitRank comm 0x14f65fa0 rank 4 nranks 8 cudaDev 0 nvmlDev 1 busId 41000 commId 0x17a6476d46efe9d9 - Init COMPLETE
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/home/users/industry/ai-hpc/apacsc32/scratch/workdir/llama/litgpt.py312/lib/python3.12/site-packages/lightning/fabric/strategies/model_parallel.py:535: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
[x1000c3s3b0n0:828713] 15 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[x1000c3s3b0n0:828713] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096
Verifying settings ...
Epoch 1 | iter 1 step 1 | loss train: 1.417, val: n/a | iter time: 5379.64 ms (step)
Epoch 1 | iter 1 step 1 | loss train: 1.428, val: n/a | iter time: 5390.40 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.627, val: n/a | iter time: 5081.73 ms (step)
Epoch 1 | iter 2 step 2 | loss train: 1.435, val: n/a | iter time: 5086.31 ms (step)
Epoch 1 | iter 3 step 3 | loss train: 1.331, val: n/a | iter time: 5092.07 ms (step)
Epoch 1 | iter 3 step 3 | loss train: 1.288, val: n/a | iter time: 5086.58 ms (step)
Epoch 1 | iter 4 step 4 | loss train: 1.010, val: n/a | iter time: 5083.41 ms (step)
Epoch 1 | iter 4 step 4 | loss train: 0.962, val: n/a | iter time: 5085.00 ms (step)
Epoch 2 | iter 5 step 5 | loss train: 0.880, val: n/a | iter time: 5258.94 ms (step)
Epoch 2 | iter 5 step 5 | loss train: 0.834, val: n/a | iter time: 5258.90 ms (step)
Training time: 28.07s
Memory used: 17.73 GB
Training time: 28.10s
Memory used: 17.73 GB
x1000c3s4b0n0:2898258:2898878 [3] NCCL INFO [Service thread] Connection closed by localRank 3
x1000c3s4b0n0:2898256:2898876 [1] NCCL INFO [Service thread] Connection closed by localRank 1
x1000c3s3b0n0:828731:829309 [3] NCCL INFO [Service thread] Connection closed by localRank 3
x1000c3s4b0n0:2898257:2898875 [2] NCCL INFO [Service thread] Connection closed by localRank 2
x1000c3s4b0n0:2898255:2898877 [0] NCCL INFO [Service thread] Connection closed by localRank 0
x1000c3s3b0n0:828729:829308 [1] NCCL INFO [Service thread] Connection closed by localRank 1
x1000c3s3b0n0:828730:829307 [2] NCCL INFO [Service thread] Connection closed by localRank 2
x1000c3s3b0n0:828728:829311 [0] NCCL INFO [Service thread] Connection closed by localRank 0
x1000c3s3b0n0:828731:832787 [3] NCCL INFO comm 0x14117290 rank 3 nranks 8 cudaDev 3 busId 81000 - Abort COMPLETE
x1000c3s4b0n0:2898258:2902251 [3] NCCL INFO comm 0x14a1e7b0 rank 7 nranks 8 cudaDev 3 busId 81000 - Abort COMPLETE
x1000c3s3b0n0:828730:832789 [2] NCCL INFO comm 0x1558c8c0 rank 2 nranks 8 cudaDev 2 busId 3000 - Abort COMPLETE
x1000c3s3b0n0:828729:832788 [1] NCCL INFO comm 0x1413d660 rank 1 nranks 8 cudaDev 1 busId c1000 - Abort COMPLETE
x1000c3s4b0n0:2898256:2902252 [1] NCCL INFO comm 0x152e6ff0 rank 5 nranks 8 cudaDev 1 busId c1000 - Abort COMPLETE
x1000c3s4b0n0:2898257:2902254 [2] NCCL INFO comm 0x143701b0 rank 6 nranks 8 cudaDev 2 busId 3000 - Abort COMPLETE
x1000c3s4b0n0:2898255:2902253 [0] NCCL INFO comm 0x14f65fa0 rank 4 nranks 8 cudaDev 0 busId 41000 - Abort COMPLETE
x1000c3s3b0n0:828728:832790 [0] NCCL INFO comm 0x161490e0 rank 0 nranks 8 cudaDev 0 busId 41000 - Abort COMPLETE
